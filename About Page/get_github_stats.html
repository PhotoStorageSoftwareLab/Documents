<!DOCTYPE html>
<html>
	<head>
		<title>About Page</title>
		<style>
		p{
			 width: 750px;
		}
		</style>
	</head>
	<script>
	function successCallBackCommits(result){
		document.getElementById("commits").innerHTML = "Total Commits: " + result.total_count
		var github_ids = {
			"adripedroza": 0,
			"abhimand": 0,
			"shash-pandey27": 0,
			"mmachado33": 0,
			"pranav12321": 0,
			"umer936": 0,
			"bpham1": 0
		};
		
		result.items.forEach(commit_func)
		
		function commit_func(item) {
			github_ids[item.author["login"]] += 1
		}
		
		for (var key in github_ids) {
			document.getElementById("commits_" + key).innerHTML = "Total Commits: " + github_ids[key]
		}
	}
	function successCallBackIssues(result){
		document.getElementById("issues").innerHTML = "Total Issues: " + result.total_count
		var github_ids = {
			"adripedroza": 0,
			"abhimand": 0,
			"shash-pandey27": 0,
			"mmachado33": 0,
			"pranav12321": 0,
			"umer936": 0,
			"bpham1": 0
		};
		
		result.items.forEach(issue_func)
		
		function issue_func(item) {
			github_ids[item.user["login"]] += 1
		}
		
		for (var key in github_ids) {
			document.getElementById("issues_" + key).innerHTML = "Total Issues: " + github_ids[key]
		}
	}
	async function getIssues() {
		const url = "https://api.github.com/search/issues?q=repo:PhotoStorageSoftwareLab/NextcloudWebApp"
		const headers = {
			"Accept" : "application/vnd.github.machine-man-preview"
		}
		const response = await fetch(url, {
			"method" : "GET",
			"headers" : headers
		})
		const result = response.json()
		
		console.log(result)
		result.then(successCallBackIssues)
	}
	async function getCommits() {
		const url = "https://api.github.com/search/commits?q=repo:PhotoStorageSoftwareLab/NextcloudWebApp author-date:2019-09-01..2020-12-30"
		const headers = {
			"Accept" : "application/vnd.github.cloak-preview"
		}
		const response = await fetch(url, {
			"method" : "GET",
			"headers" : headers
		})
		const result = response.json()
		
		console.log(result)
		result.then(successCallBackCommits)
	}
	</script>
	
	<body>
		<h1>Description</h1>
		
		<h1>Group: Crimson</h1>
		
		<h1>Group Members</h1>
		
		<h2>Adriana Pedroza</h2>
		<h3>Bio</h3>
		<p>I am a senior student at UT Austin, specializing in Software Engineering and Data Science. 
			I have industry experience with robotic process automation, natural language processing, and serverless solutions including AWS.<br>
			My most familiar languages are Java and Javascript, though I have experience with Python, C/C++/C#, and specialized languages like SQL. I collect stickers and playing cards, and have a twin sister studying MechE at A&M!</p>
		<h3>Major</h3>
		<p>Computer Engineering</p>
		<h3>Responsibilities</h3>
		<p>Selenium Testing; Front-end design</p>
		<h3>Github Stats</h3>
		<p id="commits_adripedroza"></p>
		<p id="issues_adripedroza"></p>
		<p>Total unit tests: </p>
		
		<h2>Abhishek Mandal</h2>
		<h3>Bio</h3>
		<h3>Major</h3>
		<h3>Responsibilities</h3>
		<h3>Github Stats</h3>
		<p id="commits_abhimand"></p>
		<p id="issues_abhimand"></p>
		<p>Total unit tests: </p>
		
		<h2>Shashwat Pandey</h2>
		<h3>Bio</h3>
		<h3>Major</h3>
		<h3>Responsibilities</h3>
		<h3>Github Stats</h3>
		<p id="commits_shash-pandey27"></p>
		<p id="issues_shash-pandey27"></p>
		<p>Total unit tests: </p>
		
		<h2>Matthew Machado</h2>
		<h3>Bio</h3>
		<p>I am a senior doing a dual degree, studying Software Engineering and Japanese. 
			I interned at USAA last Summer and focused on testing in Java/Kotlin. 
			I'm most familiar with Java/Kotlin, but have experience in Python and C/C++ as well. </p>
		<h3>Major</h3>
		<p> Electrical and Computer Engineering and Asian Cultures and Languages - Japanese </p>
		<h3>Responsibilities</h3>
		<p> Front-end design and testing </p>
		<h3>Github Stats</h3>
		<p id="commits_mmachado33"></p>
		<p id="issues_mmachado33"></p>
		<p>Total unit tests: </p>
		
		<h2>Pranav Rama</h2>
		<h3>Bio</h3>
		<p>I am a senior specializing in Data Science and Communication Networks.
			I'm familiar with Java, Python, C, C++ and Rust </p>
		<h3>Major</h3>
		<p>Electrical and Computer Engineering</p>
		<h3>Responsibilities</h3>
		<p>Data Scraping, integration of Yolo</p>
		<h3>Github Stats</h3>
		<p id="commits_pranav12321"></p>
		<p id="issues_pranav12321"></p>
		<p>Total unit tests: </p>
		
		<h2>Umer Salman</h2>
		<h3>Bio</h3>
		<h3>Major</h3>
		<h3>Responsibilities</h3>
		<h3>Github Stats</h3>
		<p id="commits_umer936"></p>
		<p id="issues_umer936"></p>
		<p>Total unit tests: </p>
		
		<h2>Brandon Pham</h2>
		<img src="bpham.jpg">
		<h3>Bio</h3>
		<p style="width:750px">
		I am a senior student studying Software Engineering and Network Security. 
		I am specialized in Software Development, versed in Python, Java, and C#.
		I interned at AMD last Summer and Spring. I strive for a job in Network or Cyber security.
		</p>
		<h3>Major</h3>
		<p>Electrical and Computer Engineering</p>
		<h3>Responsibilities</h3>
		<p>Data Scraping</p>
		<h3>Github Stats</h3>
		<p id="commits_bpham1"></p>
		<p id="issues_bpham1"></p>
		<p>Total unit tests: 0</p>
		
		<h1>The Github Repository</h1>
		<a href="https://github.com/PhotoStorageSoftwareLab/NextcloudWebApp">Repo</a>
		<p id="commits">Total Commits: </p>
		<p id="issues">Total Issues: </p>
		
		<script>
		getCommits()
		getIssues()
		</script>
		
		<h1>Data</h1>
		<a href='https://www.alltopsights.com/'>All Top Sights</a>
		<p style="width:750px">This site has a list of popular locations and their Latitude and Longitude filtered by city.
		The HTML tables on this site can be easily extracted (they are simple HTML tables).
		However, to get all locations, we need to programmatically shift through multiple pages
		since each page is limited to a single city. We can use a list of cities from the next sources to achieve this.
		This can be done using Python and Scrapy.</p>
		
		<a href="http://www.geonames.org/search.html?q=&country=US">Geonames</a>
		<p style="width:750px">This site has a list of cities and their Latitude and Longitude filtered by country.
		The HTML tables on this site can be easily extracted (they are simple HTML tables).
		However, to get all cities, we need to programmatically shift through multiple pages
		since each page is limited to 50 cities. This can be done using Python and Scrapy.</p>
		
		<a href="https://developers.google.com/public-data/docs/canonical/countries_csv">Countries CSV</a>
		<p style="width:750px">This site has a list of countries and their Latitude and Longitude.
		The HTML tables on this site can be easily extracted (they are simple HTML tables).
		Since all information is on a single page, Excel can be used scrape all the data easily to a csv.
		Data > Get Data > From Other Sources > From Web</p>
		
		<h1>Tools</h1>
		<p>The primary tool used in this project is Nextcloud. Nextcloud will provide local storage to store photos on a machine in the user’s local network. These photos will be uploaded through a dedicated web application. This web application will be created with a combination of Javascript and React, built for a Node environment. Image processing will be handled through YOLO, a C++ image recognition system. YOLO comes with a neural network named darknet developed by the same person. Additionally, YOLO has default weights with classifications for over 9000 objects. The YOLO associated set of tools handles this part for us so we do not need to reinvent the wheel. Nextcloud’s web application toolkit also includes PHP (with PHPUnit for unit testing) and MySQL for the backend.
		Because most of the front-end programming is built for the front-end, we intend to use web development tools that are included in the browser of choice. We may also utilize Selenium for comprehensive design tests.
		We also are using tools for us to manage a project of this scope. We will be keeping our code on GitHub in a group so that we can have multiple repositories for different aspects of the project. Additionally, all non-code files, such as this report, are kept on a team Google Drive (semi-ironic). Lastly, we have created a Slack group with dedicated channels for things like the application development vs general communication. 
		For Data Scraping, we created a scraping script in Python using a data scraping module called Scrapy.
		</p>

	</body>
</html>
